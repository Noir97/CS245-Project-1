# CS245-Project-1
Seed-based Weakly Supervised Named Entity Recognition




<b> Setup for Tensorflow-Named Entity Recognition </b>

Note 

Utilizes tensorflow 1.13.1, but requires python 3.7.9 or lower. Can utilize pyenv to update local python enviornment.

Can either follow steps within the readme, but here's a summary
- Install tf_metrics
- Within the datafile (in our case testdata1) run make download-glove, will get the vector file
- An existing dataset exists within testdata1, you can change if wished, but if you just want to test the existing dataset, just directly run make build.

- Current implementation only uses lstf_crf_ema model, but for more models check out https://github.com/guillaumegenthial/tf_ner
- Go to the models/lstf_crf_ema folder, and run python main.py (This will train the bi-lstm + crf on the dataset)
- This will generate a results folder that will include the score inside and have the predictions for each train/test dataset.
- To get recall,precision and f1 score, run ../conlleval < results/score/{name}.preds.txt > results/score/score.{name}.metrics.txt

- To generate a predicted tags file from an input, go to interact.py and change the INPUT_FILE. Updated to direclty read in a text file and return an output_tags.txt file.

<b> Generated phrases from Twitter</b>


We put a example zip file (2020-03-22_clean-hydrated.zip) in the repository. This is the compressed result from one of our text file (2020-03-22_clean-hydrated.txt) as a example. We used Autophrase to mine the phrases from 7 text files like this. Due to the github size restrictions, we can't upload everything related to AutoPhrase to make it run. However, the process of running autoPhrase as follow:

1. put your txt file into data/EN (in this case would be 2020-03-22_clean-hydrated.txt)
2. run auto_phrase.sh
3. find your result in model/DBLP, notice that the Autophrase.txt is the file we need(this file includes salient phrases).
4. Do a parsing to remove the scores in each line, and remove all phrases with 3 or more words.
If you want to run AutoPhrase with full experience, please pull its original repo https://github.com/shangjingbo1226/AutoPhrase/tree/master/src and replace auto_phrase.sh with the one shown in our repository.

We put the generated AutoPhrase.txt, rename those, in a folder called AutoPhrase result. This is the result doing step 1 to 3. We also include a parsing file to do step 4 in the same folder.

We put all the phrases generated by AutoPhrase mining the Twitter texts into the folder called phrase_list. Each file inside the folder is named after the date
of the original full twitter text file(i.e: 0322 stands for twitter recorded on 03/22/2020)

<b> CoNLL 2003 data</b>


We use CoNLL 2003 data as a small trial on using Spacy or BERT. The folder has train/test/validation split and the python code to parse the training/testing data
to the format accepted by Bi-LSTM-CRF is there. Using parsing_annotated_conll.py would retain the original tag of the input data and parse it to 2 files, one 
for sentence and one for tag. As these 2 files are required for Bi-LSTM model. parsing_conll_entity would do the tagging based on the expanded entity set. This
Should be only used for CoNLL training set to cover the original tags.

<b>Spacy expansion</b>

<b>Final Input generation</b>

<b> Evaluations and result</b> 
