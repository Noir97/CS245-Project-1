# -*- coding: utf-8 -*-

#phrases generated by AutoPhrase for conll dataset
phrases_list_path = "../phrase_list/conll_list.txt"
#dictionary with name of countries and sampled city names
dict_path="./data/dictionary_sampled.txt"
#candiddate_path is where you write your result of expansion to
candidate_path = "./data/BERT_cand_conll.txt"

import numpy as np
import torch
import nltk
from nltk.corpus import stopwords
import ssl
from transformers import BertTokenizer, BertModel

try:
  _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
	  pass
else:
  ssl._create_default_https_context = _create_unverified_https_context
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))


model=BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model.eval()
from scipy.spatial.distance import cosine

"""Given that we can't create word embeddings on the Twitter corpus due to the large RAM requirement, we would consider the BERT embeddings already trained on Wikipedia (as noted on Hugging face) """

count = 0
def get_word_embeding(word):
  words = tokenizer.tokenize(word)
  # print(word)
  word1_index = tokenizer.convert_tokens_to_ids(words)
  # print(word1_index)
  segments_ids = [1] * len(word1_index)
  word1_index = torch.tensor([word1_index])
  segments_tensors = torch.tensor([segments_ids])
  with torch.no_grad():
    embedding1 = model(word1_index, segments_tensors)[2]
    embedding1 = torch.stack(embedding1, dim=0)
    embedding1 = torch.squeeze(embedding1, dim=1)
    sum_vec = torch.sum(embedding1[-4:], dim=0)
    ave_vec = torch.mean(sum_vec, dim=0)
  global count
  count = count + 1
  if count % 100 == 0:
    print(count)
  return ave_vec

phrases_list=[]
phrase_embeddings=[]
with open(phrases_list_path,'r',encoding='ascii',errors='ignore') as f:
  phrases_list = f.readlines()
f.close()
for i in range(len(phrases_list)):
  phrases_list[i] = phrases_list[i].rstrip()
  tmp = ''.join([j for j in phrases_list[i] if not j.isdigit()])
  phrases_list[i] = " ".join(tmp.split())

#remove phrase with more than 2 words
for i in range(len(phrases_list) - 1, -1, -1):
  if phrases_list[i].count(' ') > 1:
    phrases_list.remove(phrases_list[i])

#remove phrase with weird char
for i in range(len(phrases_list) - 1, -1, -1):
  if '\u200b' in phrases_list[i]:
    phrases_list.remove(phrases_list[i])

  if phrases_list[i] in stop_words:
    phrases_list.remove(phrases_list[i])
  #remove all single char phrase
  if len(phrases_list[i])==1:
    phrases_list.remove(phrases_list[i])

#filter all None in the list
phrases_list = list(filter(None, phrases_list))

phrase_embeddings = list(map(get_word_embeding,phrases_list))

phrase_embedding_t = torch.stack(phrase_embeddings)

dict_phrase = []
dict_embeddings = []
with open(dict_path,'r',encoding='utf-8') as f:
  dict_phrase=f.readlines()
f.close()

for i in range(len(dict_phrase)):
  dict_phrase[i]=dict_phrase[i].rstrip()
  dict_phrase[i]=dict_phrase[i].rstrip('\n')
  #might be improtant to make everything in lower case
  dict_phrase[i]=dict_phrase[i].lower()

dict_embeddings = list(map(get_word_embeding,dict_phrase))
dict_embeddings_t = torch.stack(dict_embeddings)

#generated candidates are stored in phrase_set
from scipy.spatial.distance import cosine

#only compare the phrases/words with the same amount of token
#i.e: unigram to unigram, bigram to bigram
phrase_set = set()
for i in range(0, len(dict_phrase)):
  single_dict_embedding = dict_embeddings_t[i]
  similarity_score = []
  num_words=dict_phrase[i].count(' ')
  for j in range(0, len(phrases_list)):
    if phrases_list[j].count(" ")!=num_words:
      similarity_score.append(0)
      continue
    single_phrase_embedding = phrase_embedding_t[j]
    similarity = 1 - cosine(single_dict_embedding, single_phrase_embedding)
    similarity_score.append(similarity)
  similarity_score = np.array(similarity_score)
  index_best = np.argsort(similarity_score)
  index_best = index_best[::-1]
  #filter the data with 
  for n in range(0, 5):
    temp_res=phrases_list[index_best[n]]
    temp_score=similarity_score[index_best[n]]
    if temp_res not in dict_phrase and temp_score>=0.80:
      phrase_set.add(phrases_list[index_best[n]])
  if i % 100 == 0:
    print(i)

candidates = list(phrase_set)

#add the dictionary path to it
candidates=candidates+dict_phrase

#save all the candidates on candidate path
with open(candidate_path,'w',encoding='utf-8') as f:
  for line in candidates:
    f.write(line+'\n')
f.close()

